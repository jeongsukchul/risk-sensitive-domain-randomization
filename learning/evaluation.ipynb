{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "os.environ['XLA_FLAGS'] = xla_flags\n",
    "# os.environ[\"MUJOCO_GL\"] = \"egl\"\n",
    "os.environ['JAX_PLATFORM_NAME'] = 'gpu'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # change to your GPU id\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from helper import parse_cfg\n",
    "from omegaconf import OmegaConf\n",
    "from flax.training import checkpoints\n",
    "\n",
    "# import mediapy as media\n",
    "import hydra\n",
    "import functools\n",
    "from brax.training.acme import running_statistics\n",
    "from etils import epath\n",
    "from omegaconf import OmegaConf\n",
    "from typing import Any, Callable, Dict, Optional, Type, Union, Tuple\n",
    "from mujoco import mjx\n",
    "import numpy\n",
    "from custom_envs import registry, dm_control_suite, locomotion\n",
    "import jax\n",
    "from learning.agents.ppo import networks as ppo_networks\n",
    "from learning.configs import dm_control_training_config, locomotion_training_config\n",
    "from custom_envs import mjx_env\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from learning.agents.sampler_ppo.train import _unpmap\n",
    "from learning.module.wrapper.adv_wrapper import wrap_for_adv_training\n",
    "from learning.module.wrapper.evaluator import AdvEvaluator\n",
    "\n",
    "\n",
    "def evaluate(cfg, key):\n",
    "    num_eval_envs = 4096\n",
    "    # Load environment\n",
    "    env = registry.load(cfg.task)\n",
    "    env_cfg = registry.get_default_config(cfg.task)\n",
    "\n",
    "\n",
    "    print(\"env nv\", env._mj_model.nv)\n",
    "    obs_size = env.observation_size\n",
    "    act_size = env.action_size\n",
    "    rng = jax.random.PRNGKey(cfg.eval_seed)\n",
    "    randomizer = registry.get_domain_randomizer_ood(cfg.task) if cfg.ood_setting else registry.get_domain_randomizer_eval(cfg.task)\n",
    "    v_randomization_fn = functools.partial(\n",
    "        randomizer,\n",
    "        dr_range= env.ood_range if cfg.ood_setting else (env.dr_range_wide if cfg.dr_wide else env.dr_range) ,\n",
    "    )\n",
    "    dr_range_low, dr_range_high = env.ood_range if cfg.ood_setting else (env.dr_range_wide if cfg.dr_wide else env.dr_range) \n",
    "    eval_env = wrap_for_adv_training(\n",
    "        env,\n",
    "        episode_length=env_cfg.episode_length,\n",
    "        action_repeat=env_cfg.action_repeat,\n",
    "        randomization_fn=v_randomization_fn,\n",
    "        param_size = len(dr_range_low),\n",
    "        dr_range_low=dr_range_low,\n",
    "        dr_range_high=dr_range_high,\n",
    "    )  # pytype: disable=wrong-keyword-args\n",
    "    \n",
    "\n",
    "    if \"ppo\" in cfg.policy:\n",
    "        if cfg.task in dm_control_suite._envs:\n",
    "            ppo_params = dm_control_training_config.brax_ppo_config(cfg.task)\n",
    "        elif cfg.task in locomotion._envs:\n",
    "            ppo_params = locomotion_training_config.locomotion_ppo_config(cfg.task)\n",
    "        network_factory = ppo_networks.make_ppo_networks\n",
    "        if \"network_factory\" in ppo_params:\n",
    "            network_factory = functools.partial(\n",
    "                ppo_networks.make_ppo_networks,\n",
    "                **ppo_params.network_factory\n",
    "            )\n",
    "        ppo_network = network_factory(\n",
    "            observation_size=obs_size,\n",
    "            action_size=act_size,\n",
    "            preprocess_observations_fn=running_statistics.normalize if ppo_params.normalize_observations else None,\n",
    "        )\n",
    "\n",
    "        make_policy_fn = ppo_networks.make_inference_fn(ppo_network)\n",
    "    # Load saved parameters\n",
    "    save_dir = os.path.join(cfg.work_dir, \"models\")\n",
    "    print(f\"Loading parameters from {save_dir}\")\n",
    "    with open(os.path.join(save_dir, f\"{cfg.policy}_params_latest.pkl\"), \"rb\") as f:\n",
    "        params = pickle.load(f)\n",
    "    eval_key, key = jax.random.split(key)\n",
    "    evaluator = AdvEvaluator(\n",
    "        eval_env,\n",
    "        functools.partial(make_policy_fn, deterministic=True),\n",
    "        num_eval_envs=num_eval_envs,\n",
    "        episode_length=env_cfg.episode_length,\n",
    "        action_repeat=env_cfg.action_repeat,\n",
    "        key=eval_key,\n",
    "    )\n",
    "    if len(dr_range_low) > 2:\n",
    "        param_key, key = jax.random.split(key)\n",
    "        dynamics_params_grid = jax.random.uniform(param_key, shape=(4096, len(dr_range_low)), minval=dr_range_low, maxval=dr_range_high)\n",
    "        metrics, reward_1d, epi_length = evaluator.run_evaluation(\n",
    "            params,\n",
    "            dynamics_params=dynamics_params_grid,\n",
    "            training_metrics={},\n",
    "            num_eval_seeds=10,\n",
    "            success_threshold=0.7,\n",
    "        )\n",
    "        return metrics, reward_1d\n",
    "\n",
    "    elif len(dr_range_low) == 2:\n",
    "        x, y = jnp.meshgrid(jnp.linspace(dr_range_low[0], dr_range_high[0], 64),\\\n",
    "                              jnp.linspace(dr_range_low[1], dr_range_high[1], 64))\n",
    "        dynamics_params_grid = jnp.c_[x.ravel(), y.ravel()]\n",
    "        metrics, reward_1d, epi_length = evaluator.run_evaluation(\n",
    "            params,\n",
    "            dynamics_params=dynamics_params_grid,\n",
    "            training_metrics={},\n",
    "            num_eval_seeds=10,\n",
    "            success_threshold=0.7,\n",
    "        )\n",
    "        \n",
    "        # --- PLOTTING SECTION ---\n",
    "        eval_fig = plt.figure()\n",
    "        reward_2d = reward_1d.reshape(x.shape)\n",
    "        \n",
    "        # Define the threshold you want to visualize (e.g., 500, or a value from cfg)\n",
    "        boundary_threshold = 600\n",
    "        \n",
    "        vmin, vmax = 0, 1000\n",
    "        \n",
    "        import numpy as np\n",
    "        import matplotlib.patches as patches\n",
    "        \n",
    "        # 1. Plot the filled contours (Heatmap)\n",
    "        levels = np.linspace(vmin, vmax, 11)\n",
    "        ctf = plt.contourf(x, y, reward_2d, levels=levels, cmap='viridis')\n",
    "        cbar = eval_fig.colorbar(ctf, ticks=levels)\n",
    "        \n",
    "        # 2. Add the specific boundary line for the threshold\n",
    "        # We check if the threshold is within the data range to avoid errors\n",
    "        if np.min(reward_2d) < boundary_threshold < np.max(reward_2d):\n",
    "            boundary_line = plt.contour(\n",
    "                x, y, reward_2d, \n",
    "                levels=[boundary_threshold], \n",
    "                colors='white',       # High contrast color\n",
    "                linestyles='dashed',  # Distinct style\n",
    "                linewidths=2\n",
    "            )\n",
    "            # Optional: Add label to the line\n",
    "            plt.clabel(boundary_line, inline=True, fontsize=10, fmt=f'Thresh: {boundary_threshold}')\n",
    "\n",
    "        # 3. Add the nominal range rectangle\n",
    "        nominal_low, nominal_high = env.dr_range\n",
    "        width = nominal_high[0] - nominal_low[0]\n",
    "        height = nominal_high[1] - nominal_low[1]\n",
    "        xlabel, ylabel = env.ood_label if cfg.ood_setting else env.dr_label\n",
    "        plt.xlabel(xlabel) \n",
    "        plt.ylabel(ylabel)\n",
    "        if cfg.dr_wide:\n",
    "            rect = patches.Rectangle(\n",
    "                (nominal_low[0], nominal_low[1]),\n",
    "                width,\n",
    "                height,\n",
    "                linewidth=2,\n",
    "                edgecolor='r',\n",
    "                facecolor='none',\n",
    "                linestyle='--',\n",
    "                label='Trained Range'\n",
    "            )\n",
    "            plt.gca().add_patch(rect)\n",
    "            plt.legend(loc='upper left')\n",
    "        eval_fig.suptitle(f\"Evaluation on Each Params\")\n",
    "        eval_fig.tight_layout()\n",
    "        eval_fig.canvas.draw()\n",
    "        \n",
    "        return metrics, reward_2d, eval_fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfg_path= epath.Path(\".\").resolve()\n",
    "print(cfg_path)\n",
    "cfg_path = os.path.join(cfg_path, \"config.yaml\")\n",
    "# cfg = compose(config_name=\"config.yaml\")\n",
    "cfg = OmegaConf.load(cfg_path)\n",
    "cfg.task=\"CheetahRun\"\n",
    "cfg.policy=\"ppo\"\n",
    "cfg.seed=1\n",
    "print(\"cfg:\", cfg)\n",
    "cfg = parse_cfg(cfg)\n",
    "if cfg.policy ==\"ppo\":\n",
    "    cfg.work_dir = f\"./logs/{cfg.task}/{cfg.seed}/{cfg.policy}\"\n",
    "elif cfg.policy ==\"gmmppo\":\n",
    "    cfg.work_dir = f\"./logs/{cfg.task}/{cfg.seed}/{cfg.policy}/beta={cfg.beta}\"\n",
    "elif cfg.policy =='epoptppo':\n",
    "    cfg.work_dir = f\"./logs/{cfg.task}/{cfg.seed}/{cfg.policy}/epsilon={cfg.epsilon}\"\n",
    "cfg.dr_wide=False\n",
    "cfg.ood_setting=True\n",
    "cfg.eval_seed= 53\n",
    "print(\"work dir\", cfg.work_dir)\n",
    "\n",
    "metrics, reward_2d, fig = evaluate(cfg, jax.random.PRNGKey(cfg.eval_seed))\n",
    "\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfg_path= epath.Path(\".\").resolve()\n",
    "print(cfg_path)\n",
    "cfg_path = os.path.join(cfg_path, \"config.yaml\")\n",
    "# cfg = compose(config_name=\"config.yaml\")\n",
    "cfg = OmegaConf.load(cfg_path)\n",
    "cfg.task=\"CheetahRun\"\n",
    "cfg.policy=\"gmmppo\"\n",
    "cfg.seed=2\n",
    "cfg.beta=-20\n",
    "print(\"cfg:\", cfg)\n",
    "cfg = parse_cfg(cfg)\n",
    "if cfg.policy ==\"ppo\":\n",
    "    cfg.work_dir = f\"./logs/{cfg.task}/{cfg.seed}/{cfg.policy}\"\n",
    "elif cfg.policy ==\"gmmppo\":\n",
    "    cfg.work_dir = f\"./logs/{cfg.task}/{cfg.seed}/{cfg.policy}/beta={cfg.beta}\"\n",
    "elif cfg.policy =='epoptppo':\n",
    "    cfg.work_dir = f\"./logs/{cfg.task}/{cfg.seed}/{cfg.policy}/epsilon={cfg.epsilon}\"\n",
    "cfg.dr_wide=False\n",
    "cfg.ood_setting=True\n",
    "cfg.eval_seed=23\n",
    "print(\"work dir\", cfg.work_dir)\n",
    "\n",
    "metrics, reward_2d, fig = evaluate(cfg, jax.random.PRNGKey(cfg.eval_seed))\n",
    "\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfg_path= epath.Path(\".\").resolve()\n",
    "print(cfg_path)\n",
    "cfg_path = os.path.join(cfg_path, \"config.yaml\")\n",
    "# cfg = compose(config_name=\"config.yaml\")\n",
    "cfg = OmegaConf.load(cfg_path)\n",
    "cfg.task=\"CheetahRun\"\n",
    "cfg.policy=\"epoptppo\"\n",
    "cfg.seed=1\n",
    "cfg.beta=-20\n",
    "cfg.epsilon=0.4\n",
    "print(\"cfg:\", cfg)\n",
    "cfg = parse_cfg(cfg)\n",
    "if cfg.policy ==\"ppo\":\n",
    "    cfg.work_dir = f\"./logs/{cfg.task}/{cfg.seed}/{cfg.policy}\"\n",
    "elif cfg.policy ==\"gmmppo\":\n",
    "    cfg.work_dir = f\"./logs/{cfg.task}/{cfg.seed}/{cfg.policy}/beta={cfg.beta}\"\n",
    "elif cfg.policy =='epoptppo':\n",
    "    cfg.work_dir = f\"./logs/{cfg.task}/{cfg.seed}/{cfg.policy}/epsilon={cfg.epsilon}\"\n",
    "cfg.dr_wide=False\n",
    "cfg.ood_setting=True\n",
    "cfg.eval_seed= 56\n",
    "print(\"work dir\", cfg.work_dir)\n",
    "\n",
    "metrics, reward_2d, fig = evaluate(cfg, jax.random.PRNGKey(cfg.eval_seed))\n",
    "\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfg_path= epath.Path(\".\").resolve()\n",
    "print(cfg_path)\n",
    "cfg_path = os.path.join(cfg_path, \"config.yaml\")\n",
    "# cfg = compose(config_name=\"config.yaml\")\n",
    "cfg = OmegaConf.load(cfg_path)\n",
    "cfg.task=\"CheetahRun\"\n",
    "cfg.policy=\"adrppo\"\n",
    "cfg.seed=0\n",
    "cfg.epsilon=0.4\n",
    "print(\"cfg:\", cfg)\n",
    "cfg = parse_cfg(cfg)\n",
    "if cfg.policy ==\"ppo\" or cfg.policy==\"adrppo\" or cfg.policy==\"\":\n",
    "    cfg.work_dir = f\"./logs/{cfg.task}/{cfg.seed}/{cfg.policy}\"\n",
    "elif cfg.policy ==\"gmmppo\":\n",
    "    cfg.work_dir = f\"./logs/{cfg.task}/{cfg.seed}/{cfg.policy}/beta={cfg.beta}\"\n",
    "elif cfg.policy =='epoptppo':\n",
    "    cfg.work_dir = f\"./logs/{cfg.task}/{cfg.seed}/{cfg.policy}/epsilon={cfg.epsilon}\"\n",
    "cfg.dr_wide=False\n",
    "cfg.ood_setting=True\n",
    "cfg.eval_seed= 56\n",
    "print(\"work dir\", cfg.work_dir)\n",
    "\n",
    "metrics, reward_2d, fig = evaluate(cfg, jax.random.PRNGKey(cfg.eval_seed))\n",
    "\n",
    "metrics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
