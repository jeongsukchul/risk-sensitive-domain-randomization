{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e0722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import os\n",
    "\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] =\"false\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8a649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import jax\n",
    "import jax.random as jr\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "import optax\n",
    "from learning.module.bijx.autoregressive import make_autoregressive_nsf_bijx\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data_key, flow_key, train_key, sample_key = jr.split(jr.key(0), 4)\n",
    "rngs = nnx.Rngs(params=0, sample=1)\n",
    "ndim=2\n",
    "bins = 16  # same as Zuko's `bins`\n",
    "hidden = (64, 64)\n",
    "n_transforms = 3\n",
    "low = jnp.array([-.0, -.0])\n",
    "high = jnp.array([1, 1.0])\n",
    "seed= 10\n",
    "low = jnp.array([-3.0, -3.0])\n",
    "high = jnp.array([3, 3.0])\n",
    "# low = jnp.array([-7.0, -7.0])\n",
    "# high = jnp.array([7, 7.0])\n",
    "dist_bijx = make_autoregressive_nsf_bijx(\n",
    "    ndim=ndim,\n",
    "    bins=16,\n",
    "    n_transforms=3,\n",
    "    seed=seed,\n",
    "    domain_range=(low, high),\n",
    ")\n",
    "\n",
    "import optax\n",
    "tx = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),\n",
    "    optax.adamw(1e-3, weight_decay=1e-4), # Adds weight decay for regularization\n",
    ")\n",
    "optimizer = nnx.Optimizer(\n",
    "    dist_bijx,\n",
    "    tx,\n",
    "    wrt=nnx.Param,\n",
    ")\n",
    "\n",
    "# Sampling:\n",
    "key = jax.random.PRNGKey(seed)\n",
    "contexts, logq = dist_bijx.sample(batch_shape=(1024,), rng=key)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.hist2d(contexts[:, 0], contexts[:, 1], bins=100, range=[[low[0]-1, high[0]+1], [low[1]-1, high[1]+1]])\n",
    "plt.title(\"Initial flow samples\")\n",
    "plt.show()\n",
    "logq2 = dist_bijx.log_density(x=contexts)\n",
    "jnp.abs(logq-logq2).max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41179064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax, jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import optax\n",
    "def target0(contexts, target=jnp.array([0.3, 0.7]), sigma=0.12, noise_std=0.2, key=None):\n",
    "    # reward in [0,1], peaked near target\n",
    "    diff = contexts - target[None, :]\n",
    "    r = jnp.exp(-0.5 * jnp.sum((diff / sigma) ** 2, axis=-1))\n",
    "    if key is not None:\n",
    "        r = jnp.clip(r + noise_std * jax.random.normal(key, r.shape), 0.0, 1.0)\n",
    "    logr =  jnp.log(r + 1e-10)\n",
    "    return jax.nn.log_softmax(logr)\n",
    "def target1(x):\n",
    "    # Mixture of two Gaussians\n",
    "    mean1 = jnp.array([1.0, 0.4])\n",
    "    cov1 = 0.3 * jnp.array([[1.0, 0.3], [0.3, 1.0]])\n",
    "\n",
    "    mean2 = jnp.array([-1.0, -0.4])\n",
    "    cov2 = 0.1 * jnp.array([[1.0, -0.3], [-0.3, 1.0]])\n",
    "\n",
    "    pdf1 = jax.scipy.stats.multivariate_normal.pdf(x, mean1, cov1)\n",
    "    pdf2 = jax.scipy.stats.multivariate_normal.pdf(x, mean2, cov2)\n",
    "\n",
    "    return jnp.log(0.4 * pdf1 + 0.6 * pdf2)\n",
    "def target2(z):\n",
    "    z1, z2 = jnp.split(z, 2, axis=-1)\n",
    "    norm = jnp.sqrt(z1 ** 2 + z2 ** 2)\n",
    "\n",
    "    exp1 = jnp.exp(-0.5 * ((z1 - 2) / 0.8) ** 2)\n",
    "    exp2 = jnp.exp(-0.5 * ((z1 + 2) / 0.8) ** 2)\n",
    "    u = 0.5 * ((norm - 4) / 0.4) ** 2 - jnp.log(exp1 + exp2)\n",
    "\n",
    "    return -u\n",
    "\n",
    "beta = -10\n",
    "gamma = .0\n",
    "def target3(z):\n",
    "    m=3\n",
    "    r0=0.65\n",
    "    sr=0.12\n",
    "    X, Y = jnp.split(z, 2, axis=-1)\n",
    "    x = 2.0 * (X - 0.5)\n",
    "    y = 2.0 * (Y - 0.5)\n",
    "    r = jnp.hypot(x, y)\n",
    "    theta = jnp.arctan2(y, x)\n",
    "\n",
    "    ring = jnp.exp(-0.5 * ((r - r0) / sr) ** 2)\n",
    "    petals = jnp.cos(m * theta)\n",
    "    U = jnp.tanh(1.6 * (ring * petals))  # bounded in (-1, 1)\n",
    "    log_unnorm = -beta * U.squeeze() \n",
    "    regularization_weight = .01\n",
    "    log_prior = -0.5 * regularization_weight * jnp.sum(z**2, axis=-1)\n",
    "\n",
    "    # --- 3. Combine ---\n",
    "    return jax.nn.log_softmax(log_unnorm) #stabilization\n",
    "target = target1\n",
    "\n",
    "volume = jnp.prod(high - low)\n",
    "N = 2 **10\n",
    "@nnx.jit\n",
    "def train_step(model, model_old, optimizer, key):\n",
    "    kr, kc1, kc2 = jax.random.split(key, 3)\n",
    "    graphdef, prev_state = nnx.split(model)\n",
    "    contexts, logq = model.sample((N,), rng=kc1)\n",
    "    contexts = jax.random.uniform(kr, shape=(N, 2), minval=low, maxval=high)\n",
    "    # logp = target(contexts)\n",
    "    def loss_fn(model):\n",
    "        # contexts, logq = model.sample((N,), rng=kc1)\n",
    "        logq = model.log_density(x=contexts)\n",
    "        logp = target(contexts)\n",
    "        \n",
    "        # return (logq-logp).mean()\n",
    "        log_weights = logq - logp# jax.lax.stop_gradient(logp)\n",
    "        log_weights = jax.nn.log_softmax(log_weights)\n",
    "        weights = jnp.exp(log_weights)\n",
    "        denom = jnp.sum(weights)\n",
    "        importance_weights = jax.lax.stop_gradient(weights / denom)\n",
    "        original_loss = (importance_weights* logq).mean()\n",
    "        return original_loss\n",
    "        value_loss = - volume * (logq *jnp.exp(logq) * logp).mean()\n",
    "        entropy_loss = volume * (jnp.exp(logq) * logq).mean()\n",
    "        return value_loss + entropy_loss\n",
    "        # --- Proximal Term (KL Divergence) ---\n",
    "        # KL(q_old || q_current) = E_old [ log_q_old - log_q_current ]\n",
    "        # We stop gradient on logq_old because we don't update the old model\n",
    "        samples_prev, logq_old = model_old.sample((1000,), rng=kc2)\n",
    "        logq_current = model.log_density(x=samples_prev)\n",
    "        kl_div = jax.lax.stop_gradient(logq_old) - logq_current\n",
    "        proximal_loss = gamma * jnp.mean(kl_div)\n",
    "        \n",
    "        return original_loss + proximal_loss\n",
    "\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
    "    optimizer.update(model=model, grads=grads)\n",
    "    model_old = nnx.merge(graphdef, prev_state)\n",
    "    return loss\n",
    "\n",
    "def run_toy_corner(model, flow_opt):\n",
    "    x, y = jnp.meshgrid(jnp.linspace(low[0], high[0], 100), jnp.linspace(low[1], high[1], 100))\n",
    "    grid = jnp.c_[x.ravel(), y.ravel()]\n",
    "    pdf_values = target(grid)\n",
    "    pdf_values = jnp.exp(pdf_values)\n",
    "    pdf_values = jnp.reshape(pdf_values, x.shape)\n",
    "    T = 3000\n",
    "    snap_iters = [0, 1, 2, 5, 10, 15, 24] + list(range(30, T, 100))\n",
    "    # snap_iters = []\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    gif_frames = [] \n",
    "    loss_hist = []\n",
    "    graphdef, state = nnx.split(model)\n",
    "    model_old = nnx.merge(graphdef, state)\n",
    "    for t in range(T):\n",
    "        kplot, ktrain, key = jax.random.split(key, 3)\n",
    "        loss = train_step(model, model_old, flow_opt, ktrain)\n",
    "        loss_hist.append(loss)\n",
    "        if t in snap_iters:\n",
    "            fig, (ax_reward, ax_NF) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "            ctf = ax_reward.contourf(x, y, pdf_values, levels=20)\n",
    "            ax_reward.set_title(\"target landscape\")\n",
    "            ax_reward.set_xlabel(\"context dim 1\")\n",
    "            ax_reward.set_ylabel(\"context dim 2\")\n",
    "            z, _= model.sample(batch_shape=(1024,), rng=kplot)\n",
    "            fig.colorbar(ctf, ax=ax_reward, label=\"reward\")\n",
    "            ax_NF.hist2d(z[:, 0].flatten(), z[:, 1].flatten(), (100, 100), #range=[[-3, 3], [-3, 3]])\n",
    "                    range=[[low[0], high[0]], [low[1], high[1]]])\n",
    "            ax_NF.set_title(f\"NF(Neural Spline) training distribution (iter {t})\")\n",
    "            fig.tight_layout()\n",
    "            fig.canvas.draw()\n",
    "            frame = np.asarray(fig.canvas.buffer_rgba())[..., :3]\n",
    "            gif_frames.append(frame)\n",
    "            # plt.close(fig)\n",
    "            # z, _= model.sample((2**20,), rng=kplot)  # Sample 1000 points\n",
    "            # plt.figure(figsize=(10,10))\n",
    "            # plt.hist2d(z[:, 0].flatten(), z[:, 1].flatten(), (100, 100), #range=[[-3, 3], [-3, 3]])\n",
    "            #         range=[[low[0]-1, high[0]+1], [low[1]-1, high[1]+1]])\n",
    "\n",
    "\n",
    "    if gif_frames:\n",
    "        imageio.mimsave(\"NF_NeuralSpline_training.gif\", gif_frames, fps=4)\n",
    "        print(\"Saved GIF to NF_NeuralSpline_training.gif\")\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    x, _ = model.sample(batch_shape=(2**14,), rng=key)\n",
    "    x_np = np.array(x)\n",
    "    np.save(f\"ns_samples_beta={beta}.npy\", x_np)\n",
    "    # 1) Raw scatter plot (no binning)\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.scatter(x[:, 0], x[:, 1], s=1, alpha=0.2)\n",
    "    plt.xlim(low[0], high[0])\n",
    "    plt.ylim(low[1], high[1])\n",
    "    plt.title(\"Scatter of flow samples\")\n",
    "    plt.savefig(f\"NS_image_beta={beta}.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # 2) Coarse 2D histogram to kill striping\n",
    "    # plt.figure(figsize=(5, 5))\n",
    "    # plt.hist2d(x[:, 0], x[:, 1], bins=40, range=[[low[0], high[0]], [low[1], high[1]]])\n",
    "    # plt.title(\"Flow samples hist2d, coarse bins\")\n",
    "    # plt.show()\n",
    "    loss_plot = np.array(loss_hist)\n",
    "    plt.plot(loss_plot)\n",
    "x, y = jnp.meshgrid(jnp.linspace(low[0], high[0], 100), jnp.linspace(low[1], high[1], 100))\n",
    "grid = jnp.c_[x.ravel(), y.ravel()]\n",
    "pdf_values = target(grid)\n",
    "pdf_values = jnp.exp(pdf_values)\n",
    "pdf_values = jnp.reshape(pdf_values, x.shape)\n",
    "fig = plt.figure()\n",
    "ctf = plt.contourf(x, y, pdf_values, levels=20, cmap='viridis')\n",
    "cbar = fig.colorbar(ctf)\n",
    "# ax = render_flow_pdf_2d_subplots(\n",
    "#                 log_prob_fn=lambda x : dist_bijx.log_density(x=x),\n",
    "#                 low=low,\n",
    "#                 high=high,\n",
    "#             )\n",
    "run_toy_corner(dist_bijx, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc540c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts, logq = dist_bijx.sample(batch_shape=(2048,), rng=key)\n",
    "logq2 = dist_bijx.log_density(x=contexts)\n",
    "jnp.abs(logq-logq2).sum()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
