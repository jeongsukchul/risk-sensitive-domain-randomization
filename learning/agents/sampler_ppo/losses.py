# Copyright 2025 The Brax Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Proximal policy optimization training.

See: https://arxiv.org/pdf/1707.06347.pdf
"""

from typing import Any, Tuple

from brax.training import types
from agents.sampler_ppo import networks as samplerppo_networks
from brax.training.types import Params
import flax
import jax
import jax.numpy as jnp
from learning.module import networks
from learning.module.gmmvi.network import GMMTrainingState


@flax.struct.dataclass
class SAMPLERPPONetworkParams:
  """Contains training state for the learner."""
  policy: Params
  value: Params

def compute_gae(
    truncation: jnp.ndarray,
    termination: jnp.ndarray,
    rewards: jnp.ndarray,
    values: jnp.ndarray,
    bootstrap_value: jnp.ndarray,
    lambda_: float = 1.0,
    discount: float = 0.99,
):
  """Calculates the Generalized Advantage Estimation (GAE).

  Args:
    truncation: A float32 tensor of shape [T, B] with truncation signal.
    termination: A float32 tensor of shape [T, B] with termination signal.
    rewards: A float32 tensor of shape [T, B] containing rewards generated by
      following the behaviour policy.
    values: A float32 tensor of shape [T, B] with the value function estimates
      wrt. the target policy.
    bootstrap_value: A float32 of shape [B] with the value function estimate at
      time T.
    lambda_: Mix between 1-step (lambda_=0) and n-step (lambda_=1). Defaults to
      lambda_=1.
    discount: TD discount.

  Returns:
    A float32 tensor of shape [T, B]. Can be used as target to
      train a baseline (V(x_t) - vs_t)^2.
    A float32 tensor of shape [T, B] of advantages.
  """

  truncation_mask = 1 - truncation
  # Append bootstrapped value to get [v1, ..., v_t+1]
  values_t_plus_1 = jnp.concatenate(
      [values[1:], jnp.expand_dims(bootstrap_value, 0)], axis=0
  )
  deltas = rewards + discount * (1 - termination) * values_t_plus_1 - values
  deltas *= truncation_mask

  acc = jnp.zeros_like(bootstrap_value)

  def compute_vs_minus_v_xs(carry, target_t):
    lambda_, acc = carry
    truncation_mask, delta, termination = target_t
    acc = delta + discount * (1 - termination) * truncation_mask * lambda_ * acc
    return (lambda_, acc), (acc)

  (_, _), (vs_minus_v_xs) = jax.lax.scan(
      compute_vs_minus_v_xs,
      (lambda_, acc),
      (truncation_mask, deltas, termination),
      length=int(truncation_mask.shape[0]),
      reverse=True,
  )
  # Add V(x_s) to get v_s.
  vs = jnp.add(vs_minus_v_xs, values)

  vs_t_plus_1 = jnp.concatenate(
      [vs[1:], jnp.expand_dims(bootstrap_value, 0)], axis=0
  )
  advantages = (
      rewards + discount * (1 - termination) * vs_t_plus_1 - values
  ) * truncation_mask
  return jax.lax.stop_gradient(vs), jax.lax.stop_gradient(advantages)


def compute_samplerppo_loss(
    params: SAMPLERPPONetworkParams,
    normalizer_params: Any,
    data: types.Transition,
    rng: jnp.ndarray,
    samplerppo_network: samplerppo_networks.SAMPLERPPONetworks,
    entropy_cost: float = 1e-4,
    discounting: float = 0.9,
    reward_scaling: float = 1.0,
    gae_lambda: float = 0.95,
    clipping_epsilon: float = 0.3,
    normalize_advantage: bool = True,
) -> Tuple[jnp.ndarray, types.Metrics]:
  """Computes GMMPPO loss.

  Args:
    params: Network parameters,
    normalizer_params: Parameters of the normalizer.
    data: Transition that with leading dimension [B, T]. extra fields required
      are ['state_extras']['truncation'] ['policy_extras']['raw_action']
      ['policy_extras']['log_prob']
    rng: Random key
    gmmppo_network: GMMPPO networks.
    entropy_cost: entropy cost.
    discounting: discounting,
    reward_scaling: reward multiplier.
    gae_lambda: General advantage estimation lambda.
    clipping_epsilon: Policy loss clipping epsilon
    normalize_advantage: whether to normalize advantage estimate

  Returns:
    A tuple (loss, metrics)
  """
  parametric_action_distribution = samplerppo_network.parametric_action_distribution
  policy_apply = samplerppo_network.policy_network.apply
  value_apply = samplerppo_network.value_network.apply

  # Put the time dimension first.
  data = jax.tree_util.tree_map(lambda x: jnp.swapaxes(x, 0, 1), data)
  policy_logits = policy_apply(
      normalizer_params, params.policy, data.observation
  )

  baseline = value_apply(normalizer_params, params.value, data.observation)
  terminal_obs = jax.tree_util.tree_map(lambda x: x[-1], data.next_observation)
  bootstrap_value = value_apply(normalizer_params, params.value, terminal_obs)

  rewards = data.reward * reward_scaling
  truncation = data.extras['state_extras']['truncation']
  termination = (1 - data.discount) * (1 - truncation)

  target_action_log_probs = parametric_action_distribution.log_prob(
      policy_logits, data.extras['policy_extras']['raw_action']
  )
  behaviour_action_log_probs = data.extras['policy_extras']['log_prob']

  vs, advantages = compute_gae(
      truncation=truncation,
      termination=termination,
      rewards=rewards,
      values=baseline,
      bootstrap_value=bootstrap_value,
      lambda_=gae_lambda,
      discount=discounting,
  )
  if normalize_advantage:
    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
  rho_s = jnp.exp(target_action_log_probs - behaviour_action_log_probs)

  surrogate_loss1 = rho_s * advantages
  surrogate_loss2 = (
      jnp.clip(rho_s, 1 - clipping_epsilon, 1 + clipping_epsilon) * advantages
  )

  policy_loss = -jnp.mean(jnp.minimum(surrogate_loss1, surrogate_loss2))

  # Value function loss
  v_error = vs - baseline
  v_loss = jnp.mean(v_error * v_error) * 0.5 * 0.5

  # Entropy reward
  entropy = jnp.mean(parametric_action_distribution.entropy(policy_logits, rng))
  entropy_loss = entropy_cost * -entropy

  total_loss = policy_loss + v_loss + entropy_loss
  return total_loss, {
      'total_loss': total_loss,
      'policy_loss': policy_loss,
      'v_loss': v_loss,
      'entropy_loss': entropy_loss,
  }
def make_gmm_update(gmm_network: networks.FeedForwardNetwork):
  def gmm_update(
          gmmvi_state, key
    ):
      samples, mapping, sample_dist_densities, target_lnpdfs, target_lnpdf_grads = \
          gmm_network.sample_selector.select_train_datas(gmmvi_state.sample_db_state)
      new_component_stepsizes = gmm_network.component_stepsize_fn(gmmvi_state.model_state)
      new_model_state = gmm_network.model.update_stepsizes(gmmvi_state.model_state, new_component_stepsizes)
      expected_hessian_neg, expected_grad_neg = gmm_network.more_ng_estimator(new_model_state,
                                                              samples,
                                                              sample_dist_densities,
                                                              target_lnpdfs,
                                                              target_lnpdf_grads)
      new_model_state = gmm_network.component_updater(new_model_state,
                                      expected_hessian_neg,
                                      expected_grad_neg,
                                      new_model_state.stepsizes)
          
      new_model_state = gmm_network.weight_updater(new_model_state, samples, sample_dist_densities, target_lnpdfs,
                                                      gmmvi_state.weight_stepsize)
      new_num_updates = gmmvi_state.num_updates + 1
      # new_model_state, new_component_adapter_state, new_sample_db_state = \
          # gmm_network.component_adapter(gmmvi_state.component_adaptation_state,
          #                                             gmmvi_state.sample_db_state,
          #                                             new_model_state,
          #                                             new_num_updates,
                                                      # key)

      return GMMTrainingState(temperature=gmmvi_state.temperature,
                          model_state=new_model_state,
                          component_adaptation_state=gmmvi_state.component_adaptation_state,#new_component_adapter_state,
                          num_updates=new_num_updates,
                          sample_db_state=gmmvi_state.sample_db_state,#new_sample_db_state,
                          weight_stepsize=gmmvi_state.weight_stepsize)
  return gmm_update
def flow_loss(
    flow_model,
    dynamics_params: jnp.ndarray,
    prev_sample : jnp.ndarray,
    prev_logp : jnp.ndarray,
    target_lnpdf: Any,
    key: jax.random.PRNGKey,
    dr_range_high,
    dr_range_low,
    gamma,
    entropy_update = True,
):
  """Loss for training the flow network to generate adversarial dynamics parameters."""
  # If dr_low/dr_high are 1-D (shape: [D]) â†’ scalar
  volume = jnp.prod(dr_range_high - dr_range_low)
  data_log_prob = flow_model.log_density(x=dynamics_params)
  data_log_prob = jnp.clip(data_log_prob, -1e6, 1e6)
  current_sample, current_logp = flow_model.sample((10000,), key)
  current_logp_on_prev = flow_model.log_density(x = prev_sample)
  kl_loss = (prev_logp - current_logp_on_prev).mean()
  # Get next action and log prob from policy for adversarial observation
  if entropy_update:
    value_loss = - volume * (data_log_prob *jnp.exp(data_log_prob) * target_lnpdf).mean()
    entropy_loss = volume * (jnp.exp(current_logp) * current_logp).mean()
  else:
    value_loss = (data_log_prob * target_lnpdf).mean()
    entropy_loss = (current_logp).mean()
  loss = value_loss + entropy_loss

  # Modivied with REINFORCE + Self normalization style
  log_weights = data_log_prob-target_lnpdf
  log_weights = jax.nn.softmax(log_weights)
  weights = jnp.exp(log_weights)
  weights = weights / jnp.sum(weights)
  loss = (weights * data_log_prob).mean()
  return  loss + gamma * kl_loss, ({
      'flow_value_loss': value_loss,
      'flow_entropy_loss': entropy_loss,
      'flow_kl_loss' : kl_loss,
  }, current_sample, current_logp) 

# def flow_loss(
#     flow_model,
#     dynamics_params: jnp.ndarray, # Samples x from the flow
#     prev_sample : jnp.ndarray,
#     prev_logq : jnp.ndarray,
#     target_lnpdf: jnp.ndarray,    # The non-differentiable target log p(x)
#     key,
#     gamma,
# ):
#     """
#     REINFORCE-style Loss (Score Function Estimator) for Normalizing Flow.
#     Minimizes KL(q || p) where p is a black-box target.
#     """
    
#     # 1. Calculate log q(x) for the current samples
#     # dynamics_params are fixed samples from the previous step. 
#     # We need to re-evaluate their log-prob under the *current* flow to get gradients.
#     log_q = flow_model.log_density(x=dynamics_params)
    
#     # 2. Construct the REINFORCE Signal (Advantage)
#     # Note: target_lnpdf is "log p(x)"
#     raw_signal = log_q - target_lnpdf
    
#     # 3. Variance Reduction (Baseline)
#     baseline = jnp.mean(raw_signal)
#     # advantage = raw_signal - baseline
#     advantage = raw_signal
#     advantage_detached = jax.lax.stop_gradient(advantage)
    
#     # 4. The REINFORCE Loss
#     reinforce_loss = (advantage_detached * log_q).mean()

#     # 5. KL Constraint (PPO-style / Trust Region)
#     # This keeps the new policy close to the old policy (prev_logp)
#     # If samples were generated by 'prev_sample', we evaluate them under current flow
#     current_logq_on_prev = flow_model.log_density(x=prev_sample)
#     kl_trust_region = (prev_logq - current_logq_on_prev).mean()
    
#     # 6. Optional: Explicit Entropy Regularization
#     # Note: The REINFORCE KL gradient naturally includes an entropy term.
#     # However, you can add extra entropy maximization to prevent collapse.
#     # Entropy H(q) = E [-log q]. To maximize entropy, we minimize log_q.
#     entropy_loss = log_q.mean() 
#     current_samples, curreunt_logq = flow_model.sample((10000,), key)
#     # Combine
#     total_loss = reinforce_loss + gamma * kl_trust_region
    
#     # If you wanted to explicitly weight entropy separately (common in RL, e.g. SAC):
#     # total_loss -= entropy_coeff * entropy_loss 
    
#     return total_loss, ({
#       'flow_reinforce_loss': reinforce_loss,
#       'flow_entropy_est': -entropy_loss, # Just for logging (actual entropy)
#       'flow_kl_trust' : kl_trust_region,
#       'mean_target_score': jnp.mean(target_lnpdf)
#     }, current_samples, curreunt_logq)